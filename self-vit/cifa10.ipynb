{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset and make data loader\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fc_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Fc_net, self).__init__()\n",
    "        self.fc1 = nn.Linear(3072, 4000)\n",
    "        self.fc2 = nn.Linear(4000, 2000)\n",
    "        self.fc3 = nn.Linear(2000, 1000)\n",
    "        self.fc4 = nn.Linear(1000, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Fc_net()\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    loss_sum = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"epoch\", epoch, 'acc:', correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v):\n",
    "    d_k = q.size()[-1]\n",
    "    q = q.permute(0, 2, 1)\n",
    "    k = k.permute(0, 2, 1)\n",
    "    v = v.permute(0, 2, 1)\n",
    "\n",
    "    attn_logits = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    attention = F.softmax(attn_logits, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "\n",
    "    return values, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim = 3072, nhead = 2, dropout = 0.1, batch_size = 64, seq_length = 3072):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.nhead = nhead\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = embed_dim // nhead\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        assert self.head_dim * nhead == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.in_proj_weight = Parameter(torch.empty((3 * embed_dim, embed_dim)))\n",
    "\n",
    "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, query, key, value, attn_mask = None):\n",
    "         batch_size, seq_length = query.shape\n",
    "         qkv = torch._C._nn.linear(query, self.in_proj_weight)\n",
    "         qkv = qkv.unflatten(-1, (3, self.embed_dim)).unsqueeze(0).transpose(0, -2).squeeze(-2).contiguous()\n",
    "         q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "\n",
    "         q = q.view(batch_size, self.nhead, self.head_dim)\n",
    "         k = k.view(batch_size, self.nhead, self.head_dim)\n",
    "         v = v.view(batch_size, self.nhead, self.head_dim)\n",
    "\n",
    "         values, _ = scaled_dot_product(q, k, v)\n",
    "         values = values.reshape(batch_size, seq_length)\n",
    "\n",
    "         o = self.o_proj(values)\n",
    "         return o, _\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model = 3072, nhead = 2, dim_feedforward = 3072, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=1e-5)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=1e-5)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "        self.attention = MultiheadAttention(d_model, nhead, dropout = dropout)\n",
    "\n",
    "    def forward(self, x, src_mask = None):\n",
    "        # sa\n",
    "        attn, _ = self.attention(x, x, x, attn_mask = src_mask)\n",
    "        x = x + self.dropout1(attn)\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # ff\n",
    "        x = x + self.dropout2(self.linear2(self.dropout(self.activation(self.linear1(x)))))\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        return x, _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers = 2, norm=None, ** block_args):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "        self.layers = nn.ModuleList([EncoderLayer(**block_args) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, src_mask = None):\n",
    "        attn_w = []\n",
    "        for mod in self.layers:\n",
    "            output, attention_weight = mod(x, src_mask)\n",
    "            attn_w.append(attention_weight)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output, attn_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class enhance_classifier(nn.Module):\n",
    "    def __init__(self, d_model=3072, nhead=2, num_layers=2, dim_feedforward=3072, num_classes=10):\n",
    "        super(enhance_classifier, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "\n",
    "        block_args = {'d_model': d_model, 'nhead': nhead, 'dim_feedforward': dim_feedforward, 'dropout': 0.1}\n",
    "\n",
    "        self.transformer_encoder = Encoder( num_layers = 2, **block_args)\n",
    "        self.pos_encoder = nn.Parameter(torch.zeros(1, d_model))\n",
    "        torch.nn.init.uniform_(self.pos_encoder, -0.05, 0.05)\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = x + self.pos_encoder\n",
    "        x, attn_w = self.transformer_encoder(x)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x, attn_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = enhance_classifier(d_model = 3072)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    loss_sum = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs, attn_w = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"epoch\", epoch, 'acc:', correct / total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs, _ = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
