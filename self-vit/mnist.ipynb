{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "# Load the dataset and make data loader\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v):\n",
    "    d_k = q.size()[-1]\n",
    "    attn_logits = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "    attention = F.softmax(attn_logits, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "\n",
    "    return values, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim = 784, nhead = 2, dropout = 0.1, batch_size = 64, seq_length = 784):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.nhead = nhead\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = embed_dim // nhead\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        assert self.head_dim * nhead == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.in_proj_weight = Parameter(torch.empty((3 * embed_dim, embed_dim)))\n",
    "\n",
    "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, query, key, value, attn_mask = None):\n",
    "         batch_size, seq_length = query.shape\n",
    "         qkv = torch._C._nn.linear(query, self.in_proj_weight)\n",
    "         qkv = qkv.unflatten(-1, (3, self.embed_dim)).unsqueeze(0).transpose(0, -2).squeeze(-2).contiguous()\n",
    "         q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "\n",
    "         q = q.view(batch_size, self.nhead, self.head_dim)\n",
    "         k = k.view(batch_size, self.nhead, self.head_dim)\n",
    "         v = v.view(batch_size, self.nhead, self.head_dim)\n",
    "\n",
    "         values, _ = scaled_dot_product(q, k, v)\n",
    "         values = values.reshape(batch_size, seq_length)\n",
    "\n",
    "         o = self.o_proj(values)\n",
    "         return o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model = 784, nhead = 2, dim_feedforward = 784, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=1e-5)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=1e-5)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "        self.attention = MultiheadAttention(d_model, nhead, dropout = dropout)\n",
    "\n",
    "    def forward(self, x, src_mask = None):\n",
    "        # sa\n",
    "        attn = self.attention(x, x, x, attn_mask = src_mask)\n",
    "        x = x + self.dropout1(attn)\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # ff\n",
    "        x = x + self.dropout2(self.linear2(self.dropout(self.activation(self.linear1(x)))))\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, sequence_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.sequence_length = sequence_length\n",
    "        pe = torch.zeros(sequence_length)\n",
    "        position = torch.arange(0, sequence_length, dtype=torch.float)\n",
    "        pe = position / torch.pow(10000, (2 * (position // 2)) / torch.tensor(sequence_length).float())\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)].to(x.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers = 2, norm=None, ** block_args):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "        self.layers = nn.ModuleList([EncoderLayer(**block_args) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, src_mask = None):\n",
    "        for mod in self.layers:\n",
    "            output = mod(x, src_mask)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class enhance_classifier(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model=784, nhead=2, num_layers=2, dim_feedforward=784, num_classes=10):\n",
    "        super(enhance_classifier, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = enhance_classifier()\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    loss_sum = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"epoch\", epoch, 'acc:', correct / total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Eval the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
