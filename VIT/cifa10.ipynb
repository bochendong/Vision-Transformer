{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset and make data loader\n",
    "transform = transforms.Compose(\n",
    "    [transforms.RandomCrop(32, padding=4),\n",
    "     transforms.RandomHorizontalFlip(),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2, drop_last = True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v):\n",
    "    d_k = q.size()[-1]\n",
    "\n",
    "    attn_logits = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k) # [64, 8, 5, 5]\n",
    "    attention = F.softmax(attn_logits, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "\n",
    "    return values, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, nhead, dropout, batch_size = batch_size):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.nhead = nhead\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = embed_dim // nhead\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        assert self.head_dim * nhead == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.in_proj_weight = Parameter(torch.empty((3 * embed_dim, embed_dim)))\n",
    "        nn.init.xavier_uniform_(self.in_proj_weight)\n",
    "\n",
    "        self.out_proj_weight =  Parameter(torch.empty((embed_dim, embed_dim)))\n",
    "        nn.init.xavier_uniform_(self.out_proj_weight)\n",
    "\n",
    "    def forward(self, query, key, value, attn_mask = None):\n",
    "        seq_length, batch_size, embed_dim = query.size()                                            # query size (5, 64, 256)\n",
    "\n",
    "        qkv = torch._C._nn.linear(query, self.in_proj_weight)\n",
    "\n",
    "        qkv = qkv.unflatten(-1, (3, self.embed_dim)).unsqueeze(0).transpose(0, -2).squeeze(-2).contiguous()  # qkv (5, 64, 768)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]                                                            # (5, 64, 256)\n",
    "\n",
    "        q = q.view(seq_length, batch_size * self.nhead, self.head_dim).transpose(0, 1)              # [512, 5, 32]\n",
    "        k = k.view(seq_length, batch_size * self.nhead, self.head_dim).transpose(0, 1)\n",
    "        v = v.view(seq_length, batch_size * self.nhead, self.head_dim).transpose(0, 1)\n",
    "\n",
    "        q = q.view(batch_size, self.nhead, seq_length, self.head_dim)                               # [64, 8, 5, 32]\n",
    "        k = k.view(batch_size, self.nhead, seq_length, self.head_dim)\n",
    "        v = v.view(batch_size, self.nhead, seq_length, self.head_dim)\n",
    "\n",
    "        values, _ = scaled_dot_product(q, k, v)                                                     # [64, 8, 5, 32]\n",
    "        values = values.permute(2, 0, 1, 3).contiguous().view(batch_size * seq_length, embed_dim)   # [320, 256]\n",
    "\n",
    "        o = torch._C._nn.linear(values, self.out_proj_weight)\n",
    "        o = o.view(seq_length, batch_size, embed_dim)\n",
    "\n",
    "        return o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=1e-5)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=1e-5)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "        self.attention = MultiheadAttention(d_model, nhead, dropout = dropout)\n",
    "\n",
    "    def forward(self, x, src_mask = None, src_key_padding_mask = None, is_causal = False):\n",
    "        # sa\n",
    "        sa = self.dropout1(self.attention(x, x, x, attn_mask = src_mask))\n",
    "        x = self.norm1(x + sa)\n",
    "\n",
    "        # ff\n",
    "        ff = self.dropout2(self.linear2(self.dropout(self.activation(self.linear1(x)))))\n",
    "        x = self.norm2(x + ff)\n",
    "\n",
    "        return x\n",
    "\n",
    "import copy\n",
    "\n",
    "def _get_clones(module, N):\n",
    "    # FIXME: copy.deepcopy() is not defined on nn.module\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, encoder_layer, num_layers, norm=None): #, ** block_args):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "        self.layers = _get_clones(encoder_layer, num_layers)\n",
    "        # self.layers = nn.ModuleList([EncoderLayer(**block_args) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, src_mask = None):\n",
    "        for mod in self.layers:\n",
    "            output = mod(x, src_mask)\n",
    "\n",
    "        # output = torch.nested.to_padded_tensor(output, 0.)\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels, emb_size, patch_size):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(3, emb_size, kernel_size=patch_size, stride=patch_size)\n",
    "        self.flatten = nn.Flatten(start_dim=2)\n",
    "        nn.init.kaiming_normal_(self.conv.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.conv(x)                    # (64, 256, 2, 2)\n",
    "        batch_size, c, h, w = x.shape\n",
    "        x = x.permute(2, 3, 0, 1)           # (2, 2, 64, 256)\n",
    "        x = x.view(h * w, batch_size, c)    # (4, 64, 256)\n",
    "\n",
    "        return x\n",
    "\n",
    "class SecondEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels, emb_size, patch_size):\n",
    "        super(SecondEmbedding, self).__init__()\n",
    "\n",
    "        self.conv_1 = nn.Conv2d(3, 64, kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten(start_dim=2)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.conv_1(x)\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = x.view(64, 64, 256)    # (4, 64, 256)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.positional_encodings = nn.Parameter(torch.zeros(max_len, 1, d_model), requires_grad=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        pe = self.positional_encodings[:x.shape[0]]\n",
    "        return x + pe\n",
    "\n",
    "class Classification(nn.Module):\n",
    "    def __init__(self, d_model, hidden):\n",
    "        super(Classification, self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, hidden)\n",
    "        self.act = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.act(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "class enhance_classifier(nn.Module):\n",
    "    def __init__(self, d_model = 512, nhead = 8, num_layers = 6, dim_feedforward = 512, hidden = 2048, num_classes = 10, patch_size = 16):\n",
    "        super(enhance_classifier, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "\n",
    "        block_args = {'d_model': d_model, 'nhead': nhead, 'dim_feedforward': dim_feedforward, 'dropout': 0.1}\n",
    "\n",
    "        self.patch_embedding = PatchEmbedding(in_channels = 3, emb_size = d_model, patch_size = patch_size)\n",
    "        self.second_embedding = SecondEmbedding(in_channels = 3, emb_size = d_model, patch_size = patch_size)\n",
    "        self.positional_encoding = PositionalEncoding(d_model = d_model)\n",
    "\n",
    "        encoder_layers = EncoderLayer(d_model, nhead, dim_feedforward, 0.1)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "\n",
    "        # self.transformer_encoder = Encoder(encoder_layers, num_layers = num_layers)\n",
    "\n",
    "        self.classifaction = Classification(d_model, hidden)\n",
    "        self.cls_token_emb = nn.Parameter(torch.zeros(1, 1, d_model), requires_grad=True)\n",
    "        self.ln = nn.LayerNorm([d_model])\n",
    "\n",
    "    def forward(self, x):                                               # (64, 32, 32)\n",
    "        # Add embedding\n",
    "        x = self.patch_embedding(x)                                     # torch.Size([4, 64, 256]) (h * w, batch_size, embedding dim)\n",
    "                                                                        # 4 = (32 // 16) ** 2       h = img_h // patch size\n",
    "\n",
    "        # Add positional information\n",
    "        x = self.positional_encoding(x)                                 # torch.Size([4, 64, 256])\n",
    "\n",
    "        # Add CLS token\n",
    "        cls_token_emb = self.cls_token_emb.expand(-1, x.shape[1], -1)   # torch.Size([1, 64, 256])\n",
    "        x = torch.cat([cls_token_emb, x])                               # torch.Size([5, 64, 256])\n",
    "\n",
    "        # Feed into network\n",
    "        x = self.transformer_encoder(x)                                 # torch.Size([5, 64, 256])\n",
    "\n",
    "        # Get Result\n",
    "        x = x[0]                                                        # torch.Size([64, 256])\n",
    "\n",
    "        x = self.ln(x)\n",
    "        x = self.classifaction(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = enhance_classifier(d_model = 256,\n",
    "                           dim_feedforward = 512, hidden = 512,\n",
    "                           num_layers = 6, nhead = 8,\n",
    "                           patch_size = 4)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.train()\n",
    "for epoch in range(30):\n",
    "    loss_sum = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"epoch\", epoch, 'acc:', correct / total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels, emb_size, patch_size):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(3, emb_size, kernel_size=patch_size, stride=patch_size)\n",
    "        self.flatten = nn.Flatten(start_dim=2)\n",
    "        nn.init.kaiming_normal_(self.conv.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.conv(x)                # 64, 256, 2, 2\n",
    "        print(1, x.size())\n",
    "        batch_size, c, h, w = x.shape\n",
    "        x = x.permute(2, 3, 0, 1)       # 2, 2, 64, 256\n",
    "        print(2, x.size())\n",
    "        x = x.view(h * w, batch_size, c)\n",
    "        print(3, x.size())              # 4, 64, 256\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.rand((64, 3, 32, 32))\n",
    "\n",
    "conv_1 = nn.Conv2d(3, 64, kernel_size=2, stride=2)\n",
    "\n",
    "flatten = nn.Flatten(start_dim=2)\n",
    "\n",
    "x = conv_1(test)\n",
    "\n",
    "print(x.size())\n",
    "x = flatten(x)\n",
    "print(x.size())\n",
    "\n",
    "x = x.view(64, 64, 256)    # (4, 64, 256)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
